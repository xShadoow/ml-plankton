{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68e70897",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Plancton Images Classifier</span>\n",
    "*von Bourges Julian, Hahn Sandro, Schmalzl Maximilian*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46211ff",
   "metadata": {},
   "source": [
    "Definition: \n",
    "Plankton ist die Bezeichnung für die Gesamtheit der Organismen, die im Wasser von Meeren, Flüssen und Seen leben. Sie bewegen sich nicht oder nur sehr wenig aus eigener Kraft, weshalb deren Schwimmrichtung von der Strömung vorgegeben wird."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7766c0b5",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(166, 212, 46)\">I. Daten-Aufbereitung</span>\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d909b6",
   "metadata": {},
   "source": [
    "Unser Datensatz \"Converted\" besteht aus 219 063 .png Dateien. Jede einzelne .png bildet ein Plankton ab. Um mit diesem Datensatz arbeiten zu können, müssen die einzelnen Daten, also die .png Bilder, zunächst aufbereitet und skaliert werden. <br> \n",
    "Da Java-Code direkt in Maschinencode kompiliert wird, ist er in der Regel schneller. Unsere Daten haben wir deshalb zunächst mit Java genauer betrachtet und aufbereitet. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f4bedd",
   "metadata": {},
   "source": [
    "Um einen Eindruck von der Größe der Bilder zu bekommen haben wir mit der getPercentages()-Funktion die Anzahl der Pixel berechnet, in die die einzelnen Bilder jeweils in Höhe und Breite skaliert sind. Anschließend haben wir die Bilder mit 0 - 99, 100 - 199, ... und 1300 - 1399 Pixeln zu Gruppen zusammengefast und uns die jeweilige Repräsentation der Gruppe in Prozent ausgeben lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c1f85",
   "metadata": {
    "vscode": {
     "languageId": "java"
    }
   },
   "outputs": [],
   "source": [
    "public static void getPercentages() {\n",
    "\n",
    "        DecimalFormat df = new DecimalFormat(\"0.00\");\n",
    "        int sum = 0;\n",
    "\n",
    "        for(int entry : WIDTHS_ARR) {\n",
    "            sum += entry;\n",
    "        }\n",
    "\n",
    "        System.out.println(\"Widths:\");\n",
    "\n",
    "        for(int i = 0; i < WIDTHS_ARR.length; i++) {\n",
    "            double percent = ((double) WIDTHS_ARR[i] / sum) * 100.0D;\n",
    "            System.out.println(i + \": \" + WIDTHS_ARR[i] + \" (\" + df.format(percent) + \"%)\");\n",
    "        }\n",
    "\n",
    "        System.out.println();\n",
    "        System.out.println(\"Heights:\");\n",
    "\n",
    "        for(int i = 0; i < HEIGHTS_ARR.length; i++) {\n",
    "            double percent = ((double) HEIGHTS_ARR[i] / sum) * 100.0D;\n",
    "            System.out.println(i + \": \" + HEIGHTS_ARR[i] + \" (\" + df.format(percent) + \"%)\");\n",
    "        }\n",
    "\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55032600",
   "metadata": {},
   "source": [
    "Die getPercentages() Funktion erzeugt folgenden Output (ersichtlicher dargestellt in einer Tabelle): "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34ea03",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th>Widths</th>\n",
    "    <th>Counts</th>\n",
    "    <th></th>\n",
    "    <th>Heights</th>\n",
    "    <th>Counts</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td>21592 (9,86%)</td>\n",
    "    <td></td>\n",
    "    <td>0</td>\n",
    "    <td>19926 (9,10%)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>72812 (33,24%)</td>\n",
    "    <td></td>\n",
    "    <td>1</td>\n",
    "    <td>60609 (27,67%)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>93711 (42,78%)</td>\n",
    "    <td></td>\n",
    "    <td>2</td>\n",
    "    <td>99519 (45,43%)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>15954 (7,28%)</td>\n",
    "    <td></td>\n",
    "    <td>3</td>\n",
    "    <td>19706 (9,00%)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4</td>\n",
    "    <td>7324 (3,34%)</td>\n",
    "    <td></td>\n",
    "    <td>4</td>\n",
    "    <td>9451 (4,31%)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>5</td>\n",
    "    <td>3609 (1,65%)</td>\n",
    "    <td></td>\n",
    "    <td>5</td>\n",
    "    <td>4675 (2,13%)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6</td>\n",
    "    <td>1838 (0,84%)</td>\n",
    "    <td></td>\n",
    "    <td>6</td>\n",
    "    <td>2415 (1,10%)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>7</td>\n",
    "    <td>973 (0,44%)</td>\n",
    "    <td></td>\n",
    "    <td>7</td>\n",
    "    <td>1297 (0,59%)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>8</td>\n",
    "    <td>558 (0,25%)</td>\n",
    "    <td></td>\n",
    "    <td>8</td>\n",
    "    <td>692 (0,32%)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>9</td>\n",
    "    <td>317 (0,14%)</td>\n",
    "    <td></td>\n",
    "    <td>9</td>\n",
    "    <td>380 (0,17%)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>10</td>\n",
    "    <td>213 (0,10%)</td>\n",
    "    <td></td>\n",
    "    <td>10</td>\n",
    "    <td>287 (0,13%)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>11</td>\n",
    "    <td>74 (0,03%)</td>\n",
    "    <td></td>\n",
    "    <td>11</td>\n",
    "    <td>47 (0,02%)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>12</td>\n",
    "    <td>34 (0,02%)</td>\n",
    "    <td></td>\n",
    "    <td>12</td>\n",
    "    <td>20 (0,01%)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>13</td>\n",
    "    <td>53 (0,02%)</td>\n",
    "    <td></td>\n",
    "    <td>13</td>\n",
    "    <td>38 (0,02%)</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d2cb22",
   "metadata": {},
   "source": [
    "Wir sehen, dass, sowohl in Breite und Höhe, die meist repräsentierte Gruppe 200 - 299 Pixel groß ist. Dicht gefolgt von der Gruppe mit 100 - 199 Pixeln. Alle anderen Gruppen beinhalten relativ wenige Bilder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc807d31",
   "metadata": {},
   "source": [
    "Im nächsten Schritt haben wir alle .pngs deshalb auf Höhe und Breite 200 runter- beziehungsweise hochskaliert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3673746d",
   "metadata": {
    "vscode": {
     "languageId": "java"
    }
   },
   "outputs": [],
   "source": [
    "public static void resizeAll() throws Exception {\n",
    "\n",
    "    final String inputRootPath = \"D://KI_Plankton//Plankton_Converted//Converted\";\n",
    "    final String outputRootPath = \"D://KI_Plankton//Output200//\";\n",
    "    final int targetWidth = 200;\n",
    "    final int targetHeight = 200;\n",
    "\n",
    "    File rootDir = new File(inputRootPath);\n",
    "    int i = 0;\n",
    "\n",
    "    for(String pictureName : rootDir.list()) {\n",
    "\n",
    "        File input = new File(inputRootPath + \"//\" + pictureName);\n",
    "        File output = new File(outputRootPath + pictureName);\n",
    "        resizeImage(targetWidth, targetHeight, input, output);\n",
    "\n",
    "        if(++i % 1_000 == 0) {\n",
    "            System.out.println(\"processed \" + i + \" pictures\");\n",
    "        }\n",
    "\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "public static void resizeImage(int targetWidth, int targetHeight, File input, File output) throws Exception {\n",
    "\n",
    "    BufferedImage bufferedImage = ImageIO.read(input);\n",
    "\n",
    "    boolean fitHeight = bufferedImage.getHeight() > bufferedImage.getWidth();\n",
    "    Scalr.Mode scaleMode = fitHeight ? Scalr.Mode.FIT_TO_HEIGHT : Scalr.Mode.FIT_TO_WIDTH;\n",
    "\n",
    "    BufferedImage scaledImage = resizeImage(bufferedImage, targetWidth, targetHeight, scaleMode);\n",
    "\n",
    "    int y_val = (int) ((targetHeight/2.0D) - (scaledImage.getHeight()/2.0D));\n",
    "    int x_val = (int) ((targetWidth/2.0D) - (scaledImage.getWidth()/2.0D));\n",
    "\n",
    "    BufferedImage newImage = new BufferedImage(targetWidth, targetHeight, BufferedImage.TYPE_INT_RGB);\n",
    "    Graphics2D graphics2D = newImage.createGraphics();\n",
    "    graphics2D.setPaint(new Color(0, 0, 0));\n",
    "    graphics2D.fillRect(0, 0, newImage.getWidth(), newImage.getHeight());\n",
    "\n",
    "    if(fitHeight) {\n",
    "        graphics2D.drawImage(scaledImage, x_val, 0, null);\n",
    "    } else {\n",
    "        graphics2D.drawImage(scaledImage, 0, y_val, null);\n",
    "    }\n",
    "\n",
    "    graphics2D.dispose();\n",
    "    ImageIO.write(newImage, \"png\" , output);\n",
    "\n",
    "}\n",
    "\n",
    "public static BufferedImage resizeImage(BufferedImage originalImage, int targetWidth, int targetHeight, Scalr.Mode mode) throws Exception {\n",
    "    Scalr.Method method = Scalr.Method.QUALITY;\n",
    "    return Scalr.resize(originalImage, method, mode, targetWidth, targetHeight, Scalr.OP_ANTIALIAS);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e918f4a3",
   "metadata": {},
   "source": [
    "Da jetzt alle Bilder auf dieselbe Größe skaliert wurden, können wir uns weiter Gedanken um den Test-Train Split machen. Im Converted-Ordner sind die einzelnen Bilder jeweils mit Test und Training gelabelt. Wir schauen uns genauer an, welche und wie viele Bilder wie gelabelt sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcb18d3",
   "metadata": {
    "vscode": {
     "languageId": "java"
    }
   },
   "outputs": [],
   "source": [
    "public static void printNameDetails() {\n",
    "\n",
    "    File dir = new File(PATH);\n",
    "    String[] names = dir.list();\n",
    "\n",
    "    Map<String, Integer> trainingMap = new HashMap<>();\n",
    "    Map<String, Integer> testMap = new HashMap<>();\n",
    "\n",
    "    for(String name : names) {\n",
    "\n",
    "        if(name.contains(\"training\")) {\n",
    "\n",
    "            int cutFrom = 9;\n",
    "            int cutTo = name.indexOf(\"roi\") - 1;\n",
    "            String cutName = name.substring(cutFrom, cutTo);\n",
    "\n",
    "            if(trainingMap.containsKey(cutName)) {\n",
    "                trainingMap.put(cutName, trainingMap.get(cutName) + 1);\n",
    "            } else {\n",
    "                trainingMap.put(cutName, 1);\n",
    "            }\n",
    "\n",
    "        } else {\n",
    "\n",
    "            int cutFrom = name.indexOf('_') + 1;\n",
    "            int cutTo = name.indexOf(\"roi\") - 1;\n",
    "            String cutName = name.substring(cutFrom, cutTo);\n",
    "\n",
    "            if(testMap.containsKey(cutName)) {\n",
    "                testMap.put(cutName, testMap.get(cutName) + 1);\n",
    "            } else {\n",
    "                testMap.put(cutName, 1);\n",
    "            }\n",
    "\n",
    "        }\n",
    "\n",
    "    }\n",
    "\n",
    "    System.out.println(\"--- training ---\");\n",
    "        trainingMap.entrySet().stream().sorted(Collections.reverseOrder(Map.Entry.comparingByValue())).forEach(System.out::println);\n",
    "        System.out.println(trainingMap.values().stream().mapToInt(Integer::intValue).summaryStatistics());\n",
    "        System.out.println();\n",
    "        System.out.println(\"--- test ---\");\n",
    "        testMap.entrySet().stream().sorted(Collections.reverseOrder(Map.Entry.comparingByValue())).forEach(System.out::println);\n",
    "        System.out.println(testMap.values().stream().mapToInt(Integer::intValue).summaryStatistics());\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea6fe80",
   "metadata": {},
   "source": [
    "Die Funktion printNameDetails() erzeugt folgenden Output (siehe externe Datei \"table_names\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb093689",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th>Training</th>\n",
    "    <th>Counts</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>snow</td>\n",
    "    <td>81523</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>pluteus</td>\n",
    "    <td>17558</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>diatoms</td>\n",
    "    <td>8335</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>copepods</td>\n",
    "    <td>2695</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>worms</td>\n",
    "    <td>2510</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>blurry</td>\n",
    "    <td>1416</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>appendicularia_house</td>\n",
    "    <td>999</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>noctiluca</td>\n",
    "    <td>995</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>polychaeta</td>\n",
    "    <td>957</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>mnemiopsis</td>\n",
    "    <td>882</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>pteropods</td>\n",
    "    <td>701</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>appendicularia</td>\n",
    "    <td>650</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>unknown</td>\n",
    "    <td>608</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>bipinnaria</td>\n",
    "    <td>565</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>eggs</td>\n",
    "    <td>496</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>medusae</td>\n",
    "    <td>470</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>malacostraca</td>\n",
    "    <td>449</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>zoea_larvae</td>\n",
    "    <td>327</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>rod</td>\n",
    "    <td>315</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>veliger_larvae</td>\n",
    "    <td>297</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>amphipods</td>\n",
    "    <td>288</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>larvae</td>\n",
    "    <td>275</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>phaeocystis</td>\n",
    "    <td>267</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>actinotrocha</td>\n",
    "    <td>248</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>pilidium</td>\n",
    "    <td>169</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>echinodermata</td>\n",
    "    <td>119</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "IntSummaryStatistics{count=26, sum=124114, min=119, average=4773,615385, max=81523}\n",
    "\n",
    "---\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Test</th>\n",
    "    <th>Counts</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>snow</td>\n",
    "    <td>61772</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>pluteus</td>\n",
    "    <td>11416</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>noctiluca</td>\n",
    "    <td>4064</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>diatoms</td>\n",
    "    <td>3552</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>copepods</td>\n",
    "    <td>2997</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>rod</td>\n",
    "    <td>2870</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>blurry</td>\n",
    "    <td>2428</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>worms</td>\n",
    "    <td>1655</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>appendicularia_house</td>\n",
    "    <td>1373</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>unknown</td>\n",
    "    <td>1142</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>polychaeta</td>\n",
    "    <td>401</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>appendicularia</td>\n",
    "    <td>373</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>medusae</td>\n",
    "    <td>244</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>malacostraca</td>\n",
    "    <td>152</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>mnemiopsis</td>\n",
    "    <td>152</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>bipinnaria</td>\n",
    "    <td>98</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>veliger_larvae</td>\n",
    "    <td>81</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>pilidium</td>\n",
    "    <td>75</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>zoea_larvae</td>\n",
    "    <td>59</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>larvae</td>\n",
    "    <td>23</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>actinotrocha</td>\n",
    "    <td>16</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>eggs</td>\n",
    "    <td>5</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "IntSummaryStatistics{count=22, sum=94948, min=5, average=4315,818182, max=61772}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02204ae",
   "metadata": {},
   "source": [
    "Im Trainings-Datensatz werden insgesamt 26 Spezies repräsentiert, im Test-Datensatz hingegegen nur 22. Pteropods, Amphipods, Phaeocystis und Echinodermata haben im Test-Datensatz nicht einen einzigen Repräsentanten, weshalb es keinen Sinn macht diese in unseren Datensatz mitaufzunehmen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2afbb12",
   "metadata": {},
   "source": [
    "Wir wissen auch, dass einige Spezies undersampled, also unterrepräsentiert sind (z.B. eggs, larvae, etc.). Andere dagegen sind wesentlich oversampled, also überrepräsentiert (snow, pluteus, etc.). Dies kann aus folgenden Gründen zu Problemen für unser Modell führen: \n",
    "* Manche Klassen werden mehr und besser trainiert als andere\n",
    "* Wenn eine Klasse unterrepräsentiert ist und schlecht erkannt wird, hat das kaum Einfluss auf die accuracy\n",
    "* Auf der anderen Seite kann eine Klasse die oversampled ist und besonders gut erkannt wird das Ergebnis ebenfalls verfälschen\n",
    "\n",
    "Wir müssen also gegen Over- und Undersampling vorgehen.<br> Im Trainings-Datensatz müssen genügend, und für jede Spezies etwa gleich viele, Bilder sein, damit das Modell korrekt und ausgeglichen lernen kann. Dies muss auch für den Test-Datensatz gelten, damit es zu keinen Verfälschungen bei der Auswertung kommt. Das heißt wir nehmen aus den Überrepräsentierten Spezies Bilder raus, und fügen zu den unterrepräsentierten Spezies Kopien der gegebenen Bilder hinzu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb39b7e",
   "metadata": {},
   "source": [
    "Ebenfalls muss beachtet werden, dass der Arbeitsspeicher später ein limitierender Faktor ist. Wir können kein Modell auf 220 000 Bilder trainieren ohne das der Computer schlapp macht. Aus diesem Grund haben wir uns für 1 000 Bilder pro Spezies in der Trainingsdatenmenge und 200 Bilder in der Testdatenmenge entschieden.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0eec19",
   "metadata": {},
   "source": [
    "Nach einem missglückten Versuch ist uns außerdem aufgefallen, dass die Trainings-Bilder allesamt einen Grünstich haben, während die Test-Bilder allesamt schwarz weiß sind. So können wir kein Modell genau trainieren, die Test- und Trainingsbilder unterscheiden sich maßgeblich. Deshalb müssen wir \"durchmischen\" und Bilder der Trainingsmenge in die Testmenge einbetten, und anders herum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbcd076",
   "metadata": {
    "vscode": {
     "languageId": "java"
    }
   },
   "outputs": [],
   "source": [
    "public static void overAndUnderSample() throws IOException {\n",
    "\n",
    "    final String inputRootPath = \"D://KI_Plankton//Output200\";\n",
    "    final String outputRootPath = \"D://KI_Plankton//Output200-1000//\";\n",
    "    final int targetTrainAmount = 1_000;\n",
    "    final int targetTestAmount = 200;\n",
    "\n",
    "    File rootDir = new File(inputRootPath);\n",
    "    List<PictureClass> classes = new ArrayList<>();\n",
    "\n",
    "    for(String name : rootDir.list()) {\n",
    "\n",
    "        if (name.contains(\"training\")) {\n",
    "\n",
    "            int cutFrom = 9;\n",
    "            int cutTo = name.indexOf(\"roi\") - 1;\n",
    "            String cutName = name.substring(cutFrom, cutTo);\n",
    "\n",
    "            if (cutName.equals(\"pteropods\") || cutName.equals(\"amphipods\") || cutName.equals(\"phaeocystis\") || cutName.equals(\"echinodermata\")) {\n",
    "                continue;\n",
    "            }\n",
    "\n",
    "            if (!PictureClass.contains(classes, cutName, true)) {\n",
    "                classes.add(new PictureClass(cutName, true));\n",
    "            }\n",
    "\n",
    "            PictureClass.get(classes, cutName, true).getList().add(name);\n",
    "\n",
    "        } else {\n",
    "\n",
    "            int cutFrom = name.indexOf('_') + 1;\n",
    "            int cutTo = name.indexOf(\"roi\") - 1;\n",
    "            String cutName = name.substring(cutFrom, cutTo);\n",
    "\n",
    "            if (!PictureClass.contains(classes, cutName, false)) {\n",
    "                classes.add(new PictureClass(cutName, false));\n",
    "            }\n",
    "\n",
    "            PictureClass.get(classes, cutName, false).getList().add(name);\n",
    "\n",
    "        }\n",
    "\n",
    "    }\n",
    "\n",
    "    for(PictureClass pc : classes) {\n",
    "\n",
    "        System.out.println(\"Starting on PictureClass: name=\" + pc.getName() + \", train=\" + pc.isTraining());\n",
    "\n",
    "        if(pc.isTraining()) {\n",
    "\n",
    "            if(pc.getList().size() >= targetTrainAmount) {\n",
    "\n",
    "                System.out.println(\"Using random\");\n",
    "\n",
    "                Set<Integer> rndNumbers = new Random().ints(0, pc.getList().size())\n",
    "                        .distinct()\n",
    "                        .limit(targetTrainAmount)\n",
    "                        .boxed()\n",
    "                        .collect(Collectors.toSet());\n",
    "\n",
    "                for(Integer rnd : rndNumbers) {\n",
    "\n",
    "                    String pictureName = pc.getList().get(rnd.intValue());\n",
    "                    File input = new File(inputRootPath + \"//\" + pictureName);\n",
    "                    File output = new File(outputRootPath + pictureName);\n",
    "                    FileUtils.copyFile(input, output);\n",
    "\n",
    "                }\n",
    "            } else {\n",
    "\n",
    "                System.out.println(\"Using fill\");\n",
    "\n",
    "                int copyCount = 0;\n",
    "                int currentIndex = 0;\n",
    "\n",
    "                while(copyCount < targetTrainAmount) {\n",
    "\n",
    "                    String pictureName = pc.getList().get(currentIndex);\n",
    "                    String uniqueName = pictureName.substring(0, pictureName.lastIndexOf(\".tif\")) + \"_\" + copyCount + \".tif.png\";\n",
    "                    File input = new File(inputRootPath + \"//\" + pictureName);\n",
    "                    File output = new File(outputRootPath + uniqueName);\n",
    "                    FileUtils.copyFile(input, output);\n",
    "\n",
    "                    currentIndex++;\n",
    "\n",
    "                    if(currentIndex >= pc.getList().size()) {\n",
    "                        currentIndex = 0;\n",
    "                    }\n",
    "\n",
    "                    copyCount++;\n",
    "\n",
    "                }\n",
    "\n",
    "            }\n",
    "\n",
    "        } else {\n",
    "\n",
    "            if(pc.getList().size() >= targetTestAmount) {\n",
    "\n",
    "                System.out.println(\"Using random\");\n",
    "\n",
    "                Set<Integer> rndNumbers = new Random().ints(0, pc.getList().size())\n",
    "                        .distinct()\n",
    "                        .limit(targetTestAmount)\n",
    "                        .boxed()\n",
    "                        .collect(Collectors.toSet());\n",
    "\n",
    "                for(Integer rnd : rndNumbers) {\n",
    "\n",
    "\n",
    "                    String pictureName = pc.getList().get(rnd.intValue());\n",
    "                    File input = new File(inputRootPath + \"//\" + pictureName);\n",
    "                    File output = new File(outputRootPath + pictureName);\n",
    "                    FileUtils.copyFile(input, output);\n",
    "\n",
    "                }\n",
    "\n",
    "            } else {\n",
    "\n",
    "                System.out.println(\"Using fill\");\n",
    "\n",
    "                int copyCount = 0;\n",
    "                int currentIndex = 0;\n",
    "\n",
    "                while(copyCount < targetTestAmount) {\n",
    "\n",
    "                    String pictureName = pc.getList().get(currentIndex);\n",
    "                    String uniqueName = pictureName.substring(0, pictureName.lastIndexOf(\".tif\")) + \"_\" + copyCount + \".tif.png\";\n",
    "                    File input = new File(inputRootPath + \"//\" + pictureName);\n",
    "                    File output = new File(outputRootPath + uniqueName);\n",
    "                    FileUtils.copyFile(input, output);\n",
    "\n",
    "\n",
    "                    currentIndex++;\n",
    "\n",
    "                    if(currentIndex >= pc.getList().size()) {\n",
    "                        currentIndex = 0;\n",
    "                    }\n",
    "\n",
    "                    copyCount++;\n",
    "\n",
    "                }\n",
    "\n",
    "            }\n",
    "\n",
    "        }\n",
    "\n",
    "        System.out.println(\"Finished PictureClass: name=\" + pc.getName() + \", train=\" + pc.isTraining());\n",
    "\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "public static class PictureClass {\n",
    "\n",
    "    public static PictureClass get(List<PictureClass> list, String name, boolean training) {\n",
    "        for(PictureClass pc : list) {\n",
    "            if(pc.getName().equals(name) && pc.isTraining() == training) {\n",
    "                return pc;\n",
    "            }\n",
    "        }\n",
    "        return null;\n",
    "    }\n",
    "\n",
    "    public static boolean contains(List<PictureClass> list, String name, boolean training) {\n",
    "        return get(list, name, training) != null;\n",
    "    }\n",
    "\n",
    "    private final String name;\n",
    "    private final boolean training;\n",
    "    private final List<String> list;\n",
    "\n",
    "    public PictureClass(String name, boolean training) {\n",
    "        this.name = name;\n",
    "        this.training = training;\n",
    "        this.list = new ArrayList<>();\n",
    "    }\n",
    "\n",
    "    public String getName() {\n",
    "        return this.name;\n",
    "    }\n",
    "\n",
    "    public boolean isTraining() {\n",
    "        return this.training;\n",
    "    }\n",
    "\n",
    "    public List<String> getList() {\n",
    "        return this.list;\n",
    "    }\n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b21d698",
   "metadata": {
    "vscode": {
     "languageId": "java"
    }
   },
   "outputs": [],
   "source": [
    "public static void ownTrainTestSplit() throws IOException {\n",
    "\n",
    "    final String inputRootPath = \"D://KI_Plankton//Output200\";\n",
    "    final String outputRootPath = \"D://KI_Plankton//Output200-TrainTest//\";\n",
    "    final int targetTrainAmount = 1_000;\n",
    "    final int targetTestAmount = 200;\n",
    "\n",
    "    File rootDir = new File(inputRootPath);\n",
    "    List<PictureClass> classes = new ArrayList<>();\n",
    "\n",
    "    for(String name : rootDir.list()) {\n",
    "\n",
    "        int cutFrom;\n",
    "        int cutTo = name.indexOf(\"roi\") - 1;\n",
    "\n",
    "        if(name.contains(\"training\")) {\n",
    "            cutFrom = 9;\n",
    "        } else {\n",
    "            cutFrom = name.indexOf('_') + 1;\n",
    "        }\n",
    "\n",
    "        String cutName = name.substring(cutFrom, cutTo);\n",
    "\n",
    "        if (cutName.equals(\"pteropods\") || cutName.equals(\"amphipods\") || cutName.equals(\"phaeocystis\") || cutName.equals(\"echinodermata\")) {\n",
    "            continue;\n",
    "        }\n",
    "\n",
    "\n",
    "        if (!PictureClass.contains(classes, cutName)) {\n",
    "            classes.add(new PictureClass(cutName));\n",
    "        }\n",
    "\n",
    "        PictureClass pc = PictureClass.get(classes, cutName);\n",
    "\n",
    "        if(name.contains(\"training\")) {\n",
    "            pc.getTrainingList().add(name);\n",
    "        } else {\n",
    "            pc.getTestList().add(name);\n",
    "        }\n",
    "\n",
    "    }\n",
    "\n",
    "    int targetTrainSplit = targetTrainAmount / 2;\n",
    "    int targetTestSplit = targetTestAmount / 2;\n",
    "    int pullAmount = targetTrainSplit + targetTestSplit;\n",
    "\n",
    "    for(PictureClass pc : classes) {\n",
    "\n",
    "        System.out.println(\"Starting on PictureClass name=\" + pc.getName());\n",
    "\n",
    "        List<String> tmpTrainSplit = new ArrayList<>();\n",
    "        List<String> tmpTestSplit = new ArrayList<>();\n",
    "\n",
    "        if(pc.getTrainingList().size() >= pullAmount) {\n",
    "\n",
    "            System.out.println(\"fill train list with random\");\n",
    "\n",
    "            Set<Integer> rndNumbers = new Random().ints(0, pc.getTrainingList().size())\n",
    "                    .distinct()\n",
    "                    .limit(pullAmount)\n",
    "                    .boxed()\n",
    "                    .collect(Collectors.toSet());\n",
    "\n",
    "            for(Integer rnd : rndNumbers) {\n",
    "                tmpTrainSplit.add(pc.getTrainingList().get(rnd.intValue()));\n",
    "            }\n",
    "\n",
    "        } else {\n",
    "\n",
    "            System.out.println(\"fill train list with filling\");\n",
    "\n",
    "            int copyCount = 0;\n",
    "            int currentIndex = 0;\n",
    "\n",
    "            while(copyCount < pullAmount) {\n",
    "\n",
    "                tmpTrainSplit.add(pc.getTrainingList().get(currentIndex));\n",
    "                currentIndex++;\n",
    "\n",
    "                if(currentIndex >= pc.getTrainingList().size()) {\n",
    "                    currentIndex = 0;\n",
    "                }\n",
    "\n",
    "                copyCount++;\n",
    "\n",
    "            }\n",
    "\n",
    "        }\n",
    "\n",
    "        if(pc.getTestList().size() >= pullAmount) {\n",
    "\n",
    "            System.out.println(\"fill test list with random\");\n",
    "\n",
    "            Set<Integer> rndNumbers = new Random().ints(0, pc.getTestList().size())\n",
    "                    .distinct()\n",
    "                    .limit(pullAmount)\n",
    "                    .boxed()\n",
    "                    .collect(Collectors.toSet());\n",
    "\n",
    "            for(Integer rnd : rndNumbers) {\n",
    "                tmpTestSplit.add(pc.getTestList().get(rnd.intValue()));\n",
    "            }\n",
    "\n",
    "        } else {\n",
    "\n",
    "            System.out.println(\"fill test list with filling\");\n",
    "\n",
    "            int copyCount = 0;\n",
    "            int currentIndex = 0;\n",
    "\n",
    "            while(copyCount < pullAmount) {\n",
    "\n",
    "                tmpTestSplit.add(pc.getTestList().get(currentIndex));\n",
    "                currentIndex++;\n",
    "\n",
    "                if(currentIndex >= pc.getTestList().size()) {\n",
    "                    currentIndex = 0;\n",
    "                }\n",
    "\n",
    "                copyCount++;\n",
    "\n",
    "            }\n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "        System.out.println(\"filling train and test split lists\");\n",
    "\n",
    "        List<String> firstTrainEntries = tmpTrainSplit.stream().limit(targetTrainSplit).collect(Collectors.toList());\n",
    "        List<String> firstTestEntries = tmpTestSplit.stream().limit(targetTrainSplit).collect(Collectors.toList());\n",
    "\n",
    "        List<String> lastTrainEntries = tmpTrainSplit.subList(targetTrainSplit, pullAmount);\n",
    "        List<String> lastTestEntries = tmpTestSplit.subList(targetTrainSplit, pullAmount);\n",
    "\n",
    "        List<String> trainSplit = new ArrayList<>();\n",
    "        List<String> testSplit = new ArrayList<>();\n",
    "\n",
    "        trainSplit.addAll(firstTrainEntries);\n",
    "        trainSplit.addAll(firstTestEntries);\n",
    "        testSplit.addAll(lastTrainEntries);\n",
    "        testSplit.addAll(lastTestEntries);\n",
    "\n",
    "        System.out.println(\"copying train split\");\n",
    "        int copyCount = 0;\n",
    "\n",
    "        for(String trainName : trainSplit) {\n",
    "\n",
    "            int cutFrom;\n",
    "            int cutTo = trainName.indexOf(\"roi\") - 1;\n",
    "\n",
    "            if(trainName.contains(\"training\")) {\n",
    "                cutFrom = 9;\n",
    "            } else {\n",
    "                cutFrom = trainName.indexOf('_') + 1;\n",
    "            }\n",
    "\n",
    "            String cutName = trainName.substring(cutFrom, cutTo);\n",
    "\n",
    "            if(cutName.equals(\"appendicularia\")) {\n",
    "                cutName = \"appendicularia0\";\n",
    "            } else if(cutName.equals(\"larvae\")) {\n",
    "                cutName = \"larvae0\";\n",
    "            }\n",
    "\n",
    "            String uniqueName = \"training_\" + cutName + \"_\" + copyCount + \".png\";\n",
    "            File input = new File(inputRootPath + \"//\" + trainName);\n",
    "            File output = new File(outputRootPath + uniqueName);\n",
    "            FileUtils.copyFile(input, output);\n",
    "            copyCount++;\n",
    "\n",
    "        }\n",
    "\n",
    "        System.out.println(\"copying test split\");\n",
    "        copyCount = 0;\n",
    "\n",
    "        for(String testName : testSplit) {\n",
    "\n",
    "            int cutFrom;\n",
    "            int cutTo = testName.indexOf(\"roi\") - 1;\n",
    "\n",
    "            if(testName.contains(\"training\")) {\n",
    "                cutFrom = 9;\n",
    "            } else {\n",
    "                cutFrom = testName.indexOf('_') + 1;\n",
    "            }\n",
    "\n",
    "            String cutName = testName.substring(cutFrom, cutTo);\n",
    "\n",
    "            if(cutName.equals(\"appendicularia\")) {\n",
    "                cutName = \"appendicularia0\";\n",
    "            } else if(cutName.equals(\"larvae\")) {\n",
    "                cutName = \"larvae0\";\n",
    "            }\n",
    "\n",
    "            String uniqueName = \"test_\" + cutName + \"_\" + copyCount + \".png\";\n",
    "            File input = new File(inputRootPath + \"//\" + testName);\n",
    "            File output = new File(outputRootPath + uniqueName);\n",
    "            FileUtils.copyFile(input, output);\n",
    "            copyCount++;\n",
    "\n",
    "        }\n",
    "\n",
    "        System.out.println(\"finished class\");\n",
    "\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "public static class PictureClass {\n",
    "\n",
    "    public static PictureClass get(List<PictureClass> list, String name) {\n",
    "        for(PictureClass pc : list) {\n",
    "            if(pc.getName().equals(name)) {\n",
    "                return pc;\n",
    "            }\n",
    "        }\n",
    "        return null;\n",
    "    }\n",
    "\n",
    "    public static boolean contains(List<PictureClass> list, String name) {\n",
    "        return get(list, name) != null;\n",
    "    }\n",
    "\n",
    "    private final String name;\n",
    "    private final List<String> trainingList;\n",
    "    private final List<String> testList;\n",
    "\n",
    "    public PictureClass(String name) {\n",
    "        this.name = name;\n",
    "        this.trainingList = new ArrayList<>();\n",
    "        this.testList = new ArrayList<>();\n",
    "    }\n",
    "\n",
    "    public String getName() {\n",
    "        return this.name;\n",
    "    }\n",
    "\n",
    "    public List<String> getTrainingList() {\n",
    "        return this.trainingList;\n",
    "    }\n",
    "\n",
    "    public List<String> getTestList() {\n",
    "        return this.testList;\n",
    "    }\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b08f371",
   "metadata": {},
   "source": [
    "Jetzt haben wir einen geeigneten Train-Test-Split, mit dem wir sinnvoll arbeiten können. Im Ordner \"TrainTest\" befinden sich 26 447 Planktonbilder von insgesamt 22 verschiedenen Spezies.<br>\n",
    "Davon werden 22 023 Bilder zum Training verwendet, die anderen 4 423 werden zum Testen verwendet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4144ecdc",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(166, 212, 46)\">II. Decision Trees</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7855b171",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(166, 212, 46)\">III. Convolutional Neural Network</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b51e02",
   "metadata": {},
   "source": [
    "Um einen Klassifikator zu erhalten, trainieren wir ein Convolutional Neural Network (CNN). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1462f673",
   "metadata": {},
   "source": [
    "Als aller erstes laden wir die Trainingsdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40034a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"D:/KI_Plankton/TrainTest/\"\n",
    "\n",
    "training_files = [f for f in sorted(listdir(root_path)) if \"training\" in f]\n",
    "image_list = list()\n",
    "\n",
    "for file_name in training_files:\n",
    "    #image_list.append(imread(root_path + file_name).astype('uint8')) #does not work because dtype is float (uses too much ram)\n",
    "    image_list.append(np.asarray(Image.open(root_path + file_name)))\n",
    "\n",
    "trainings_arr = np.asarray(image_list)\n",
    "print(len(trainings_arr))\n",
    "print(trainings_arr.shape)\n",
    "print(trainings_arr.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6484b7a",
   "metadata": {},
   "source": [
    "Jetzt speichern wir die Trainingsdaten in ein NumPy-Array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d33708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"D:/KI_Plankton/trainings_arr\", trainings_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1915f7",
   "metadata": {},
   "source": [
    "Als nächstes laden wir die Testdaten..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ef260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"D:/KI_Plankton/TrainTest/\"\n",
    "\n",
    "test_files = [f for f in sorted(listdir(root_path)) if \"test\" in f]\n",
    "image_list = list()\n",
    "\n",
    "for file_name in test_files:\n",
    "    #image_list.append(imread(root_path + file_name).astype('uint8')) #does not work because dtype is float (uses too much ram)\n",
    "    image_list.append(np.asarray(Image.open(root_path + file_name)))\n",
    "\n",
    "test_arr = np.asarray(image_list)\n",
    "print(len(test_arr))\n",
    "print(test_arr.shape)\n",
    "print(test_arr.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75537372",
   "metadata": {},
   "source": [
    "... und speichern auch diese in einem numPy-Array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a251135",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"D:/KI_Plankton/test_arr\", test_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603deca2",
   "metadata": {},
   "source": [
    "Jetzt mit den Trainingslabels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f69a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"D:/KI_Plankton/TrainTest/\"\n",
    "\n",
    "files = [f for f in sorted(listdir(root_path)) if \"training\" in f]\n",
    "label_list = list()\n",
    "\n",
    "for file_name in files:\n",
    "    if \"snow\" in file_name:\n",
    "        label_list.append([0])\n",
    "    elif \"pluteus\" in file_name:\n",
    "        label_list.append([1])\n",
    "    elif \"diatoms\" in file_name:\n",
    "        label_list.append([2])\n",
    "    elif \"copepods\" in file_name:\n",
    "        label_list.append([3])\n",
    "    elif \"worms\" in file_name:\n",
    "        label_list.append([4])\n",
    "    elif \"blurry\" in file_name:\n",
    "        label_list.append([5])\n",
    "    elif \"appendicularia_house\" in file_name:\n",
    "        label_list.append([6])\n",
    "    elif \"noctiluca\" in file_name:\n",
    "        label_list.append([7])\n",
    "    elif \"polychaeta\" in file_name:\n",
    "        label_list.append([8])\n",
    "    elif \"mnemiopsis\" in file_name:\n",
    "        label_list.append([9])\n",
    "    elif \"appendicularia0\" in file_name:\n",
    "        label_list.append([10])\n",
    "    elif \"unknown\" in file_name:\n",
    "        label_list.append([11])\n",
    "    elif \"bipinnaria\" in file_name:\n",
    "        label_list.append([12])\n",
    "    elif \"eggs\" in file_name:\n",
    "            label_list.append([13])\n",
    "    elif \"medusae\" in file_name:\n",
    "            label_list.append([14])\n",
    "    elif \"malacostraca\" in file_name:\n",
    "            label_list.append([15])\n",
    "    elif \"zoea_larvae\" in file_name:\n",
    "            label_list.append([16])\n",
    "    elif \"rod\" in file_name:\n",
    "            label_list.append([17])\n",
    "    elif \"veliger_larvae\" in file_name:\n",
    "            label_list.append([18])\n",
    "    elif \"larvae0\" in file_name:\n",
    "            label_list.append([19])\n",
    "    elif \"actinotrocha\" in file_name:\n",
    "            label_list.append([20])\n",
    "    elif \"pilidium\" in file_name:\n",
    "            label_list.append([21])\n",
    "    elif \"pteropods\" in file_name:\n",
    "            #label_list.append([22])\n",
    "            pass\n",
    "    elif \"amphipods\" in file_name:\n",
    "            #label_list.append([23])\n",
    "            pass\n",
    "    elif \"phaeocystis\" in file_name:\n",
    "            #label_list.append([24])\n",
    "            pass\n",
    "    elif \"echinodermata\" in file_name:\n",
    "            #label_list.append([25])\n",
    "            pass\n",
    "\n",
    "training_labels_arr = np.asarray(label_list, dtype='uint8')\n",
    "print(training_labels_arr)\n",
    "print(training_labels_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd73a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"D:/KI_Plankton/trainings_labels_arr\", training_labels_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc0c80b",
   "metadata": {},
   "source": [
    "Und den Testlabels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1667ee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"D:/KI_Plankton/TrainTest/\"\n",
    "\n",
    "files = [f for f in sorted(listdir(root_path)) if \"test\" in f]\n",
    "label_list = list()\n",
    "\n",
    "for file_name in files:\n",
    "    if \"snow\" in file_name:\n",
    "        label_list.append([0])\n",
    "    elif \"pluteus\" in file_name:\n",
    "        label_list.append([1])\n",
    "    elif \"diatoms\" in file_name:\n",
    "        label_list.append([2])\n",
    "    elif \"copepods\" in file_name:\n",
    "        label_list.append([3])\n",
    "    elif \"worms\" in file_name:\n",
    "        label_list.append([4])\n",
    "    elif \"blurry\" in file_name:\n",
    "        label_list.append([5])\n",
    "    elif \"appendicularia_house\" in file_name:\n",
    "        label_list.append([6])\n",
    "    elif \"noctiluca\" in file_name:\n",
    "        label_list.append([7])\n",
    "    elif \"polychaeta\" in file_name:\n",
    "        label_list.append([8])\n",
    "    elif \"mnemiopsis\" in file_name:\n",
    "        label_list.append([9])\n",
    "    elif \"appendicularia0\" in file_name:\n",
    "        label_list.append([10])\n",
    "    elif \"unknown\" in file_name:\n",
    "        label_list.append([11])\n",
    "    elif \"bipinnaria\" in file_name:\n",
    "        label_list.append([12])\n",
    "    elif \"eggs\" in file_name:\n",
    "            label_list.append([13])\n",
    "    elif \"medusae\" in file_name:\n",
    "            label_list.append([14])\n",
    "    elif \"malacostraca\" in file_name:\n",
    "            label_list.append([15])\n",
    "    elif \"zoea_larvae\" in file_name:\n",
    "            label_list.append([16])\n",
    "    elif \"rod\" in file_name:\n",
    "            label_list.append([17])\n",
    "    elif \"veliger_larvae\" in file_name:\n",
    "            label_list.append([18])\n",
    "    elif \"larvae0\" in file_name:\n",
    "            label_list.append([19])\n",
    "    elif \"actinotrocha\" in file_name:\n",
    "            label_list.append([20])\n",
    "    elif \"pilidium\" in file_name:\n",
    "            label_list.append([21])\n",
    "    elif \"pteropods\" in file_name:\n",
    "            #label_list.append([22])\n",
    "            pass\n",
    "    elif \"amphipods\" in file_name:\n",
    "            #label_list.append([23])\n",
    "            pass\n",
    "    elif \"phaeocystis\" in file_name:\n",
    "            #label_list.append([24])\n",
    "            pass\n",
    "    elif \"echinodermata\" in file_name:\n",
    "            #label_list.append([25])\n",
    "            pass\n",
    "\n",
    "test_labels_arr = np.asarray(label_list, dtype='uint8')\n",
    "print(test_labels_arr)\n",
    "print(test_labels_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb14c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"D:/KI_Plankton/test_labels_arr\", test_labels_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1e6a7c",
   "metadata": {},
   "source": [
    "Um mit einem Neuronalen Netz arbeiten zu können brauchen wir die labels 1-hot-encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627e3e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.load(\"D:/KI_Plankton/trainings_labels_arr.npy\")\n",
    "test_labels = np.load(\"D:/KI_Plankton/test_labels_arr.npy\")\n",
    "\n",
    "# one hot encode target values\n",
    "trainY = to_categorical(train_labels)\n",
    "testY = to_categorical(test_labels)\n",
    "\n",
    "print(np.shape(trainY))\n",
    "print(np.shape(testY))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6caf9dc",
   "metadata": {},
   "source": [
    "Jetzt laden wir noch die Bilder, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e205a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.load(\"D:/KI_Plankton/trainings_arr.npy\")\n",
    "test_images = np.load(\"D:/KI_Plankton/test_arr.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4129d3e7",
   "metadata": {},
   "source": [
    "...konvertieren in floats, normalisieren und packen sie in trainX und testX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935eb027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in floats konvertieren\n",
    "trainX = train_images.astype('float32')\n",
    "testX = test_images.astype('float32')\n",
    "\n",
    "# auf einen range von 0-1 normalisieren\n",
    "trainX = trainX / 255.0\n",
    "testX = testX / 255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c40fd",
   "metadata": {},
   "source": [
    "Jetzt können wir mit einem ersten CNN anfangen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(200, 200, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(22))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#Adam\n",
    "myAdam=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "#RMS Prop\n",
    "myRMSprop=keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "\n",
    "#Stochastic gradient descent\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) #Parameter beim SGD Lernrate, Momentum, ...\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'D:/KI_Plankton/data/train',  # this is the target directory\n",
    "        target_size=(200, 200),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'D:/KI_Plankton/data/validation',\n",
    "        target_size=(200, 200),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dad72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=22000 // batch_size,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=4400 // batch_size,\n",
    "        verbose=1,\n",
    "        callbacks=[plot_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbed69fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_generator(validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681ff2ab",
   "metadata": {},
   "source": [
    "Accuracy: 0.7736363410949707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb3840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64,                      #Anzahl der Filter\n",
    "                 kernel_size=(3,3),         #Dimensionen des Fensters\n",
    "                 strides=(1,1),             #Größe der Schritte\n",
    "                 padding='same',            #Padding: same=dim(in)=dim(out), valid=no Padding\n",
    "                 activation='relu',         #Aktivierungsfunktion\n",
    "                 input_shape=(200,200,3)))    #Dimension des Inputs)\n",
    "#model.add(Dropout(0.05))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(1, 1)))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(1, 1)))\n",
    "\n",
    "#model.add(Conv2D(64, (3, 3)))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dropout(0.05))\n",
    "model.add(Dense(22, activation='softmax'))\n",
    "\n",
    "#Adam\n",
    "myAdam=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "#RMS Prop\n",
    "myRMSprop=keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "\n",
    "#Stochastic gradient descent\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) #Parameter beim SGD Lernrate, Momentum, ...\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=myRMSprop,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        #rotation_range=10, # Rotation\n",
    "        #width_shift_range=0.2, # horizontaler Shift\n",
    "        #height_shift_range=0.2, # vertikaler Shift\n",
    "        #zoom_range=0.2, # zoom\n",
    "        #horizontal_flip=True\n",
    ")\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'D:/KI_Plankton/data/train',  # this is the target directory\n",
    "        target_size=(200, 200),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'D:/KI_Plankton/data/validation',\n",
    "        target_size=(200, 200),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "\n",
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        #steps_per_epoch=22000 // batch_size,\n",
    "        epochs=10,\n",
    "        validation_data=validation_generator,\n",
    "        #validation_steps=4400 // batch_size,\n",
    "        verbose=1,\n",
    "        callbacks=[plot_losses])\n",
    "\n",
    "\n",
    "model.evaluate_generator(validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b80b974",
   "metadata": {},
   "source": [
    "Accuracy:0.755227267742157, aber overfitted extrem! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4a7e79",
   "metadata": {},
   "source": [
    "Da das Modell overfitted ist probieren wir noch einmal ein anderes Modell und benutzen diesmal data augmentation und dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1737fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64,                      #Anzahl der Filter\n",
    "                 kernel_size=(3,3),         #Dimensionen des Fensters\n",
    "                 strides=(1,1),             #Größe der Schritte\n",
    "                 padding='same',            #Padding: same=dim(in)=dim(out), valid=no Padding\n",
    "                 activation='relu',         #Aktivierungsfunktion\n",
    "                 input_shape=(200,200,3)))    #Dimension des Inputs)\n",
    "model.add(Dropout(0.05))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(1, 1)))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(1, 1)))\n",
    "\n",
    "#model.add(Conv2D(64, (3, 3)))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(220, activation='relu'))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(22, activation='softmax'))\n",
    "\n",
    "#Adam\n",
    "myAdam=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "#RMS Prop\n",
    "myRMSprop=keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "\n",
    "#Stochastic gradient descent\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True) #Parameter beim SGD Lernrate, Momentum, ...\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=10, # Rotation\n",
    "        width_shift_range=0.2, # horizontaler Shift\n",
    "        height_shift_range=0.2, # vertikaler Shift\n",
    "        zoom_range=0.2, # zoom\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'D:/KI_Plankton/data/train',  # this is the target directory\n",
    "        target_size=(200, 200),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'D:/KI_Plankton/data/validation',\n",
    "        target_size=(200, 200),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "\n",
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=22000 // batch_size,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=4400 // batch_size,\n",
    "        verbose=1,\n",
    "        callbacks=[plot_losses])\n",
    "\n",
    "\n",
    "model.evaluate_generator(validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f4bd9e",
   "metadata": {},
   "source": [
    "Accuracy: 0.5195454359054565"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91c0bf7",
   "metadata": {},
   "source": [
    "Das geht noch besser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397aa8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained keras efficientnet\n",
    "#Defining the model\n",
    "base_model = EfficientNetB5(include_top=False, weights=\"imagenet\", input_shape=(200,200,3), classes=22)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(22, activation='softmax'))\n",
    "\n",
    "#Adam\n",
    "myAdam=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "\n",
    "#RMS Prop\n",
    "myRMSprop=keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "\n",
    "#Stochastic gradient descent\n",
    "sgd = SGD(learning_rate=0.001, decay=1e-6, momentum=0.8, nesterov=True) #Parameter beim SGD Lernrate, Momentum, ...\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=myAdam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        #rotation_range=10, # Rotation\n",
    "        #width_shift_range=0.2, # horizontaler Shift\n",
    "        #height_shift_range=0.2, # vertikaler Shift\n",
    "        #zoom_range=0.2, # zoom\n",
    "        #horizontal_flip=True\n",
    ")\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'D:/KI_Plankton/data/train',  # this is the target directory\n",
    "        target_size=(200, 200),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'D:/KI_Plankton/data/validation',\n",
    "        target_size=(200, 200),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "\n",
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        #steps_per_epoch=22000 // batch_size,\n",
    "        epochs=10,\n",
    "        validation_data=validation_generator,\n",
    "        #validation_steps=4400 // batch_size,\n",
    "        verbose=1,\n",
    "        callbacks=[plot_losses])\n",
    "\n",
    "\n",
    "model.evaluate_generator(validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91497c07",
   "metadata": {},
   "source": [
    "Hiermit haben wir unser bestes Modell, die acuracy beträgt 0.871363639831543"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c80dbdc",
   "metadata": {},
   "source": [
    "## <span style=\"color:rgb(166, 212, 46)\">IV. Conclusion</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c60f36a",
   "metadata": {},
   "source": [
    "Mit dem Convolutional Neural Network haben mir eine Methode gefunden, mit der wir die einzelnen Planktonbilder ziemlich genau ihrer korrekten Spezies zuordnen können.<br>Ganz perfekt wird und soll so ein Algorithmus wohl nie werden, mit einer accuracy von 87% sind wir aber eh sehr gut dabei.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea52d64a",
   "metadata": {},
   "source": [
    "Als abschließenden Kommentar sei gesagt, dass alle notebooks, Programme und Dateien die wir im Laufe unserer Projektarbeit genutzt und getestet haben auf Github hochgeladen sind. Zwar zugegeben nicht ganz so gut geordnet, aber hier wird noch einmal ersichtlich, wie wir vorgegangen sind und was wir alles zusätzlich zu diesem Notebook hier getestet haben. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
